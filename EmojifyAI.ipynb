{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\abdul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\abdul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re  # Import regex module for text processing\n",
    "import os  # Import os module for file and directory management\n",
    "import numpy as np  # Import NumPy for numerical operations\n",
    "import pandas as pd  # Import pandas for data manipulation and analysis\n",
    "import torch  # Import PyTorch for deep learning tasks\n",
    "from transformers import AutoTokenizer, AutoModel  # Import Hugging Face transformers for NLP models and tokenizers\n",
    "from sklearn.metrics.pairwise import cosine_similarity  # Import cosine similarity function for comparing sentence embeddings\n",
    "from nltk.corpus import stopwords  # Import stopwords from NLTK for text preprocessing\n",
    "from nltk.tokenize import word_tokenize  # Import word_tokenize from NLTK for tokenizing sentences\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AdamW, AutoTokenizer, AutoModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "\n",
    "import nltk  # Import Natural Language Toolkit (NLTK) for NLP tasks\n",
    "nltk.download('stopwords')  # Download stopwords from NLTK\n",
    "nltk.download('punkt')  # Download punkt tokenizer from NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmojiDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, tokenizer, max_length):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This code defines a class `EmojifyAI` that suggests emojis based on the input sentence. It uses a pre-trained BERT model to calculate sentence embeddings and find similarity between the input sentence and emojis' descriptions. The class also includes methods to process sentences, tokenize, and compute embeddings for sentences and emojis' descriptions. Finally, it has methods to generate and save an emoji data CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmojifyAI:  # Define the EmojifyAI class for suggesting emojis\n",
    "    def __init__(self):  # Initialize the class and load the model\n",
    "        self.load_model()\n",
    "\n",
    "    def load_model(self):  # Load the pre-trained BERT model for sentence embeddings\n",
    "        self.load_fine_tuned_model('fine_tuned_model')\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/bert-base-nli-mean-tokens')\n",
    "        #self.model = BertForSequenceClassification.from_pretrained('sentence-transformers/bert-base-nli-mean-tokens', num_labels=len(emoji_to_idx))\n",
    "\n",
    "    def process_sentence(self, sentence):  # Preprocess and tokenize input sentence\n",
    "        sentence = sentence.lower()  # Convert sentence to lowercase\n",
    "        sentence = re.sub('[^a-z]+', ' ', sentence)  # Remove non-letter characters\n",
    "        stop_words = set(stopwords.words('english'))  # Define English stopwords\n",
    "        word_tokens = word_tokenize(sentence)  # Tokenize sentence\n",
    "        sentence = [w for w in word_tokens if not w.lower() in stop_words]  # Remove stopwords\n",
    "        sentence = ' '.join(sentence)  # Join tokens to form processed sentence\n",
    "        return self.get_mean_tokens([sentence])  # Return mean tokens for the processed sentence\n",
    "\n",
    "    def process_csv(self):  # Process and filter emoji descriptions from CSV file\n",
    "        self.emoji_df = pd.read_csv(\"data/emoji-data.csv\")  # Read the emoji data CSV\n",
    "        self.all_emoji_df = self.emoji_df  # Store all emoji data\n",
    "        self.emoji_df = self.emoji_df[1800:2000]  # Select a subset of emoji data\n",
    "        self.emoji_df = self.emoji_df.reset_index(drop=True)  # Reset index\n",
    "        return self.get_mean_tokens(self.emoji_df['description'])  # Return mean tokens for emoji descriptions\n",
    "\n",
    "    def get_mean_tokens(self, sentences):  # Calculate mean tokens for sentences\n",
    "        self.obtain_tokens(sentences)  # Obtain tokens for sentences\n",
    "        self.compute_embeddings()  # Compute embeddings for tokens\n",
    "        return self.calculate_mean_value()  # Calculate and return mean values of embeddings\n",
    "\n",
    "    def obtain_tokens(self, sentences):  # Tokenize input sentences\n",
    "        self.tokens = {'input_ids': [], 'attention_mask': []}  # Initialize token dictionaries\n",
    "\n",
    "        for sentence in sentences:  # Iterate through sentences\n",
    "            new_tokens = self.tokenizer.encode_plus(sentence, max_length=128,\n",
    "                                                    truncation=True, padding='max_length',\n",
    "                                                    return_tensors='pt')  # Tokenize sentence\n",
    "            self.tokens['input_ids'].append(new_tokens['input_ids'][0])  # Append input_ids\n",
    "            self.tokens['attention_mask'].append(new_tokens['attention_mask'][0])  # Append attention_mask\n",
    "\n",
    "        self.tokens['input_ids'] = torch.stack(self.tokens['input_ids'])  # Stack input_ids\n",
    "        self.tokens['attention_mask'] = torch.stack(self.tokens['attention_mask'])  # Stack attention_mask\n",
    "\n",
    "    def compute_embeddings(self):  # Compute embeddings for tokens\n",
    "        outputs = self.model(**self.tokens)  # Forward pass through the model\n",
    "        self.embeddings = outputs.last_hidden_state  # Extract embeddings\n",
    "\n",
    "    def calculate_mean_value(self):  # Calculate mean values of embeddings\n",
    "        attention_mask = self.tokens['attention_mask']  # Retrieve attention_mask\n",
    "        mask = attention_mask.unsqueeze(-1).expand(self.embeddings.size()).float()  # Create mask\n",
    "        masked_embeddings = self.embeddings * mask  # Apply mask to embeddings\n",
    "        summed = torch.sum(masked_embeddings, 1)  # Sum masked embeddings\n",
    "        summed_mask = torch.clamp(mask.sum(1), min=1e-9)  # Sum mask values\n",
    "        self.mean_pooled = summed / summed_mask  # Calculate mean pooled embeddings\n",
    "        self.mean_pooled = self.mean_pooled.detach().numpy()  # Convert to numpy array\n",
    "        return self.mean_pooled  # Return mean pooled embeddings\n",
    "\n",
    "    def find_similarity(self, sentence_tokens, mean_tokens):  # Find similarity between sentence and emoji tokens\n",
    "        similarity = cosine_similarity([sentence_tokens], mean_tokens)  # Calculate cosine similarity\n",
    "        return similarity  # Return similarity\n",
    "\n",
    "    def generate_emoji_csv(self):  # Generate emoji data CSV file\n",
    "        df = pd.read_csv(\"data/raw-emoji-data.csv\", usecols=[1, 3], header=None)  # Read raw emoji data CSV\n",
    "        df = df.dropna()  # Drop rows with missing values\n",
    "        df = df.iloc[1:, :]  # Select all rows except the header\n",
    "        self.save_csv(df)  # Save processed data as CSV\n",
    "\n",
    "    def save_csv(self, df):  # Save emoji data to CSV file\n",
    "        df = pd.DataFrame({'emoji': df[1], 'description': df[3]})  # Create a DataFrame with emoji and description\n",
    "        df.to_csv(\"data/emoji-data.csv\", encoding='utf-8', index=False)  # Save DataFrame as CSV\n",
    "\n",
    "    def fine_tune(self, train_sentences, train_labels, val_sentences, val_labels, epochs=5, batch_size=16, learning_rate=2e-5):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        train_dataset = EmojiDataset(train_sentences, train_labels, self.tokenizer, max_length=128)\n",
    "        val_dataset = EmojiDataset(val_sentences, val_labels, self.tokenizer, max_length=128)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        optimizer = AdamW(self.model.parameters(), lr=learning_rate)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Train the model on the training set\n",
    "            correct_predictions = 0\n",
    "            self.model.train()\n",
    "            total_loss = 0\n",
    "            for batch in train_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "                #print('Working - 1')\n",
    "                self.model.zero_grad()\n",
    "                outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "            # Validate the model on the validation set\n",
    "            self.model.eval()\n",
    "            total_val_loss = 0\n",
    "            correct_predictions = 0\n",
    "            total_predictions = 0\n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    input_ids = batch['input_ids'].to(device)\n",
    "                    attention_mask = batch['attention_mask'].to(device)\n",
    "                    labels = batch['label'].to(device)\n",
    "\n",
    "                    outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                    loss = outputs.loss\n",
    "                    logits = outputs.logits\n",
    "\n",
    "                    total_val_loss += loss.item()\n",
    "                    #print('Working - 2')\n",
    "                    preds = torch.argmax(outputs.logits, dim=1)\n",
    "                    correct_predictions += torch.sum(preds == batch['label'])\n",
    "                    total_predictions += len(batch['label'])\n",
    "                    for j in range(len(preds)):\n",
    "                        pred_emoji = idx_to_emoji[preds[j].item()]\n",
    "                        true_emoji = idx_to_emoji[batch['label'][j].item()]\n",
    "                        if pred_emoji == true_emoji:\n",
    "                            correct += 1\n",
    "\n",
    "            avg_val_loss = total_val_loss / len(val_loader)\n",
    "            val_accuracy = correct_predictions / total_predictions\n",
    "\n",
    "            print(f\"Epoch: {epoch + 1}\")\n",
    "            print(f\"Train loss: {avg_train_loss}\")\n",
    "            print(f\"Validation loss: {avg_val_loss}\")\n",
    "            print(f\"Validation accuracy: {val_accuracy}\")\n",
    "\n",
    "    def save_fine_tuned_model(self, path):\n",
    "        self.model.save_pretrained(path)\n",
    "\n",
    "    def load_fine_tuned_model(self, path):\n",
    "        self.model = AutoModel.from_pretrained(path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code demonstrates how to use the EmojifyAI class to suggest emojis for a given sentence. It starts by creating an instance of the class, generating an emoji CSV file, and processing the CSV to obtain mean tokens for emojis. Then, it defines an example sentence, processes it, and finds the similarity between the sentence and emojis. It prints the top 5 most similar emojis and their descriptions. Finally, it defines a suggestEmojis function that takes a sentence as input and suggests emojis based on the similarity between the sentence and emojis' descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at fine_tuned_model were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "import pandas as pd\n",
    "\n",
    "# Read the dataset\n",
    "emoji_data = pd.read_csv(\"data/emoji-data.csv\")\n",
    "\n",
    "# Split the dataset into sentences and labels\n",
    "sentences = emoji_data['description'].tolist()\n",
    "labels = emoji_data['emoji'].tolist()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create the emoji to index and index to emoji mappings\n",
    "emoji_to_idx = {emoji: idx for idx, emoji in enumerate(set(labels))}\n",
    "idx_to_emoji = {idx: emoji for emoji, idx in emoji_to_idx.items()}\n",
    "'''\n",
    "\n",
    "emoji_rec = EmojifyAI() # Instantiate the EmojifyAI class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train loss: 7.553004243871668\n",
      "Validation loss: 7.552941508915113\n",
      "Validation accuracy: 0.0\n",
      "Epoch: 2\n",
      "Train loss: 7.426641265114585\n",
      "Validation loss: 7.690298432889192\n",
      "Validation accuracy: 0.0\n",
      "Epoch: 3\n",
      "Train loss: 7.264464058718839\n",
      "Validation loss: 7.842883980792502\n",
      "Validation accuracy: 0.0\n",
      "Epoch: 4\n",
      "Train loss: 7.107486106537201\n",
      "Validation loss: 8.050781104875648\n",
      "Validation accuracy: 0.0\n",
      "Epoch: 5\n",
      "Train loss: 6.961718920822982\n",
      "Validation loss: 8.142105102539062\n",
      "Validation accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "emoji_rec.generate_emoji_csv() # Generate the emoji data CSV file\n",
    "print(\"data done\")\n",
    "\n",
    "\n",
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split(sentences, [emoji_to_idx[l] for l in labels], test_size=0.2)\n",
    "\n",
    "emoji_rec.fine_tune(train_sentences, train_labels, val_sentences, val_labels, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_rec.save_fine_tuned_model('fine_tuned_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 768)\n"
     ]
    }
   ],
   "source": [
    "mean_tokens = emoji_rec.process_csv() # Process the CSV and obtain mean tokens for emojis\n",
    "print(mean_tokens.shape) # Print the shape of the mean tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sentence = \"Do you play games?\" # Define an example sentence\n",
    "sentence_token = emoji_rec.process_sentence(example_sentence) # Process the example sentence\n",
    "similarity = emoji_rec.find_similarity(sentence_token[0], mean_tokens) # Find the similarity between the sentence and emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 🇻🇪 flag: Venezuela\n",
      "13 🏴󠁧󠁢󠁥󠁮󠁧󠁿 flag: England\n",
      "8 🇾🇪 flag: Yemen\n",
      "6 🇼🇸 flag: Samoa\n",
      "14 🏴󠁧󠁢󠁳󠁣󠁴󠁿 flag: Scotland\n"
     ]
    }
   ],
   "source": [
    "top_indices = (-similarity[0]).argsort()[:5] # Get the indices of the top 5 most similar emojis\n",
    "for i in top_indices: # Iterate through the top indices\n",
    "    print(i, emoji_rec.emoji_df['emoji'][i], emoji_rec.emoji_df['description'][i]) # Print the index, emoji, and description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run 1 time\n",
    "# torch.save(mean_tokens, 'checkpoint/token-all.pt') # Save the mean tokens to a file named 'token-all.pt' in the 'checkpoint' directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggestEmojis(sentence): # Define the suggestEmojis function\n",
    "    name = 'token-all.pt' # Define the name of the token file\n",
    "    all_tokens = torch.load('checkpoint/'+name) # Load the precomputed tokens\n",
    "    sentence_token = emoji_rec.process_sentence(sentence) # Process the input sentence\n",
    "    similarity = emoji_rec.find_similarity(sentence_token[0], all_tokens) # Find the similarity between the sentence and emojis\n",
    "    indices = (-similarity[0]).argsort()[:5] # Get the indices of the top 5 most similar emojis\n",
    "    emoji_df = pd.read_csv(\"data/emoji-data.csv\") # Read the emoji data CSV file\n",
    "    for j in indices: # Iterate through the top indices\n",
    "        print(emoji_df['emoji'][j], emoji_df['description'][j]) # Print the emoji and its description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example - 1\n",
      "I am going to the movies\n",
      "Following are the suggested emojis:\n",
      "-----------------\n",
      "🎥 movie camera\n",
      "🎦 cinema\n",
      "📀 dvd\n",
      "📺 television\n",
      "📽 film projector\n"
     ]
    }
   ],
   "source": [
    "print(\"Example - 1\")\n",
    "test_sentence1 = \"I am going to the movies\"\n",
    "print(test_sentence1)\n",
    "print(\"Following are the suggested emojis:\\n-----------------\")\n",
    "suggestEmojis(test_sentence1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example - 2\n",
      "I love eating pizza\n",
      "Following are the suggested emojis:\n",
      "-----------------\n",
      "🍕 pizza\n",
      "😋 face savoring food\n",
      "🍟 french fries\n",
      "🍔 hamburger\n",
      "🌮 taco\n"
     ]
    }
   ],
   "source": [
    "print(\"Example - 2\")\n",
    "test_sentence2 = \"I love eating pizza\"\n",
    "print(test_sentence2)\n",
    "print(\"Following are the suggested emojis:\\n-----------------\")\n",
    "suggestEmojis(test_sentence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example - 3\n",
      "The weather is sunny today\n",
      "Following are the suggested emojis:\n",
      "-----------------\n",
      "☀ sun\n",
      "😁 beaming face with smiling eyes\n",
      "🌞 sun with face\n",
      "🌤 sun behind small cloud\n",
      "⛅ sun behind cloud\n"
     ]
    }
   ],
   "source": [
    "print(\"Example - 3\")\n",
    "test_sentence3 = \"The weather is sunny today\"\n",
    "print(test_sentence3)\n",
    "print(\"Following are the suggested emojis:\\n-----------------\")\n",
    "suggestEmojis(test_sentence3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example - 4\n",
      "I am feeling tired and sleepy\n",
      "Following are the suggested emojis:\n",
      "-----------------\n",
      "😫 tired face\n",
      "😪 sleepy face\n",
      "😩 weary face\n",
      "😞 disappointed face\n",
      "🙁 slightly frowning face\n"
     ]
    }
   ],
   "source": [
    "print(\"Example - 4\")\n",
    "test_sentence4 = \"I am feeling tired and sleepy\"\n",
    "print(test_sentence4)\n",
    "print(\"Following are the suggested emojis:\\n-----------------\")\n",
    "suggestEmojis(test_sentence4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example - 5\n",
      "My favorite sport is soccer\n",
      "Following are the suggested emojis:\n",
      "-----------------\n",
      "⚽ soccer ball\n",
      "🤟 love you gesture\n",
      "👩‍❤️‍👩 couple with heart woman woman\n",
      "😍 smiling face with heart eyes\n",
      "✌ victory hand\n"
     ]
    }
   ],
   "source": [
    "print(\"Example - 5\")\n",
    "test_sentence5 = \"My favorite sport is soccer\"\n",
    "print(test_sentence5)\n",
    "print(\"Following are the suggested emojis:\\n-----------------\")\n",
    "suggestEmojis(test_sentence5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example - 6\n",
      "Let's go to the beach this weekend\n",
      "Following are the suggested emojis:\n",
      "-----------------\n",
      "🏖 beach with umbrella\n",
      "🏕 camping\n",
      "🌇 sunset\n",
      "☀ sun\n",
      "🦪 oyster\n"
     ]
    }
   ],
   "source": [
    "print(\"Example - 6\")\n",
    "test_sentence6 = \"Let's go to the beach this weekend\"\n",
    "print(test_sentence6)\n",
    "print(\"Following are the suggested emojis:\\n-----------------\")\n",
    "suggestEmojis(test_sentence6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example - 7\n",
      "I am so excited for the party tonight\n",
      "Following are the suggested emojis:\n",
      "-----------------\n",
      "🥳 partying face\n",
      "🎉 party popper\n",
      "👏 clapping hands\n",
      "🔆 bright button\n",
      "💖 sparkling heart\n"
     ]
    }
   ],
   "source": [
    "print(\"Example - 7\")\n",
    "test_sentence7 = \"I am so excited for the party tonight\"\n",
    "print(test_sentence7)\n",
    "print(\"Following are the suggested emojis:\\n-----------------\")\n",
    "suggestEmojis(test_sentence7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example - 8\n",
      "I am working on a new project at my job\n",
      "Following are the suggested emojis:\n",
      "-----------------\n",
      "👷 construction worker\n",
      "🆕 new button\n",
      "🧑‍🏭 factory worker\n",
      "🚧 construction\n",
      "🏗 building construction\n"
     ]
    }
   ],
   "source": [
    "print(\"Example - 8\")\n",
    "test_sentence8 = \"I am working on a new project at my job\"\n",
    "print(test_sentence8)\n",
    "print(\"Following are the suggested emojis:\\n-----------------\")\n",
    "suggestEmojis(test_sentence8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example - 9\n",
      "My dog loves to play fetch\n",
      "Following are the suggested emojis:\n",
      "-----------------\n",
      "🐩 poodle\n",
      "🦮 guide dog\n",
      "🐕 dog\n",
      "🐶 dog face\n",
      "🐕‍🦺 service dog\n"
     ]
    }
   ],
   "source": [
    "print(\"Example - 9\")\n",
    "test_sentence9 = \"My dog loves to play fetch\"\n",
    "print(test_sentence9)\n",
    "print(\"Following are the suggested emojis:\\n-----------------\")\n",
    "suggestEmojis(test_sentence9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example - 10\n",
      "The traffic is terrible during rush hour\n",
      "Following are the suggested emojis:\n",
      "-----------------\n",
      "👿 angry face with horns\n",
      "🌁 foggy\n",
      "😠 angry face\n",
      "😱 face screaming in fear\n",
      "😨 fearful face\n"
     ]
    }
   ],
   "source": [
    "print(\"Example - 10\")\n",
    "test_sentence10 = \"The traffic is terrible during rush hour\"\n",
    "print(test_sentence10)\n",
    "print(\"Following are the suggested emojis:\\n-----------------\")\n",
    "suggestEmojis(test_sentence10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
